{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform & Rescale\n",
    "\n",
    "This notebook focusses on transforming and re-scaling the predictors to prevent issues of scale or skew in one or more dimensions biasing the results. The objective here is to produce two data sets that can be re-loaded and processed as many times as you like in Notebooks 7 and 8 without you ever needing to revisit this section. \n",
    "\n",
    "The **only** time you'd need to come back here is if you have decided to change the base transformation used on the scoring dimensions. In which case you need to re-run this notebook _once_ to generate new versions of the two predictor data sets. Note, however, that they will not overwrite each other, so you can (in notebooks 7 and 8) simply switch between Untransformed, Log-Transformed, and Box-Cox-Transformed data at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "r_state = 42\n",
    "random.seed(r_state) \n",
    "np.random.seed(r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "print('Your scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('Please check it is at least 0.18.0.')\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics  \n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.feature_selection import SelectKBest \n",
    "#from sklearn.feature_selection import f_regression\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analytical = os.path.join('data','analytical')\n",
    "\n",
    "def load_status_scores(dtype):\n",
    "    status = pd.read_csv(os.path.join(analytical,dtype+'-Scores.csv.gz'), index_col=0)  # SES scores\n",
    "    \n",
    "    # Scores\n",
    "    status.drop(['RANK_01','RANK_11'], axis=1, inplace=True)\n",
    "    status.rename(columns={\n",
    "        'SES_01':'SES 2001',\n",
    "        'SES_11':'SES 2011',\n",
    "        'SES_ASC':'SES Ascent 2001-2011',\n",
    "        'SES_PR_01':'SES 2001 Percentile', # 99 = High-status\n",
    "        'SES_PR_11':'SES 2011 Percentile', # 99 = High-status\n",
    "        'SES_PR_ASC':'SES Percentile Ascent 2001-2011'\n",
    "    }, inplace=True)\n",
    "    return status\n",
    "\n",
    "def load_predictors(dtype):\n",
    "    \n",
    "    return status\n",
    "\n",
    "def plot_checks(df, selected_cols, prefix='Test'):\n",
    "    sns.set(rc={\"figure.figsize\": (12, 3)})\n",
    "    for d in selected_cols:\n",
    "        print(\"Working on \" + d)\n",
    "        fig = plt.figure(d)\n",
    "        sns.distplot(df[d], color='green', hist=True, rug=True, norm_hist=False)\n",
    "        fig = plt.gcf() # *G*et the *C*urrent *F*igure environment so that the next command works\n",
    "        plt.savefig(\"{0}-{1}-Check.pdf\".format(prefix, d.replace(':',' - ')), bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    print(\"Done.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Your Transform (if any)\n",
    "\n",
    "This code enables us to switch between testing different transforms on the data. You would probably want to match what you specified in Notebook 4, though I've added all three basic outputs (Untransformed, Log, and Box-Cox) to GitHub so that it's easy to experiment with the different choices. After you've run this next section once you don't need to run it again _until_ you change the transform used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_use = 'Untransformed' # Options are: ['Untransformed','Box-Cox','Log']\n",
    "\n",
    "SES = load_status_scores(to_use)  # SES scores in 2011\n",
    "\n",
    "d01input = pd.read_csv(os.path.join('data','canonical','scores',to_use+'-Inputs-2001.csv.gz'), index_col=0)  # SES inputs\n",
    "d11input = pd.read_csv(os.path.join('data','canonical','scores',to_use+'-Inputs-2011.csv.gz'), index_col=0)  # SES inputs\n",
    "\n",
    "# Rename to remove confusion\n",
    "d01input.rename(columns=lambda x: re.sub(' 2001','',x), inplace=True)\n",
    "d11input.rename(columns=lambda x: re.sub(' 2011','',x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Read in processed datasets\n",
    "d01 = pd.read_csv(os.path.join(analytical,'Predictor-2001-Data.csv.gz'), compression='gzip', index_col=0)  #  Main dataset for 2001\n",
    "d11 = pd.read_csv(os.path.join(analytical,'Predictor-2011-Data.csv.gz'), compression='gzip', index_col=0)  #  Main dataset for 2011\n",
    "\n",
    "d01 = pd.merge(d01input, d01, how='inner', left_index=True, right_index=True)\n",
    "d11 = pd.merge(d11input, d11, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "if d01.shape[0] != 4835:\n",
    "    print(\"Wrong number of rows in d01: \" + d01.shape[0])\n",
    "if d11.shape[0] != 4835:\n",
    "    print(\"Wrong number of rows in d11: \" + d11.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Have \" + str(len(d01.columns)+1) + \" variables to work with.\")\n",
    "d01.sample(3, random_state=r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "s01 = set(d01.columns)\n",
    "s11 = set(d11.columns)\n",
    "print(\"2001 vs 2011 variable check: \" + str(s01.difference(s11)))\n",
    "print(\"2011 vs 2001 variable check: \" + str(s11.difference(s01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SES.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptives = pd.DataFrame()\n",
    "for c in d01.columns:\n",
    "    descriptives = descriptives.append(pd.concat([d01[c].describe(),d11[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives.columns = ['2001 Count','2001 Mean','2001 StD','2001 Min','2001 LQ','2001 Median','2001 UQ','2001 Max',\n",
    "                        '2011 Count','2011 Mean','2011 StD','2011 Min','2011 LQ','2011 Median','2011 UQ','2011 Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This enables to re-use the same sample below\n",
    "dsample = descriptives.sample(4, random_state=r_state).index.values\n",
    "dsample = np.append(dsample,\n",
    "                    ['Fare_Zone','House Prices',\n",
    "                     'Percentage with Level 4+ Qualifications','Percentage of Knowledge Workers',\n",
    "                     'Household Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful, but time-consuming\n",
    "#plot_checks(d01, dsample, 'Untransformed')\n",
    "descriptives[descriptives.index.isin(dsample)][\n",
    "    ['2001 Min','2011 Min','2001 Max','2011 Max','2001 Median','2011 Median']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Scaling Data\n",
    "\n",
    "In the below code the data in 2001 has unit variance scaling applied to it.  The same transformation is then applied to the data in 2011.  Finally both datasets are centred independently using median-removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Robust scaling _without_ centering\n",
    "# and _with_ common scaling. We do this \n",
    "# because 2001 and 2011 won't have the \n",
    "# same centre but we do want them to use\n",
    "# a common scale.\n",
    "rs1 = preprocessing.RobustScaler(with_centering=False, quantile_range=(25.0,75.0))\n",
    "\n",
    "#  Train on 2001 data set\n",
    "rs1.fit(d01)\n",
    "\n",
    "# Apply the same unit variance scaling to both years\n",
    "d01_trs1 = pd.DataFrame(data=rs1.transform(d01), index=d01.index, columns=d01.columns)\n",
    "d11_trs1 = pd.DataFrame(data=rs1.transform(d11), index=d11.index, columns=d11.columns)\n",
    "\n",
    "# Create new robust scaler for centering \n",
    "# _without_ common scaling.\n",
    "rs2 = preprocessing.RobustScaler(with_scaling=False)  \n",
    "\n",
    "# Centre independently\n",
    "d01_trs2 = pd.DataFrame(data=rs2.fit_transform(d01_trs1), index=d01.index, columns=d01.columns)  \n",
    "d11_trs2 = pd.DataFrame(data=rs2.fit_transform(d11_trs1), index=d11.index, columns=d11.columns)\n",
    "\n",
    "#  Write the transformed data to csv\n",
    "d01_trs2.to_csv(os.path.join(analytical,to_use+'-2001-Data-Transformed_and_Scaled.csv.gz'), compression='gzip', index=True)\n",
    "d11_trs2.to_csv(os.path.join(analytical,to_use+'-2011-Data-Transformed_and_Scaled.csv.gz'), compression='gzip', index=True) \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptives_trs1 = pd.DataFrame()\n",
    "for c in d01_trs1.columns:\n",
    "    descriptives_trs1 = descriptives_trs1.append(pd.concat([d01_trs1[c].describe(),d11_trs1[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives_trs1.columns = ['2001 Count','2001 Mean','2001 StD','2001 Min','2001 LQ','2001 Median','2001 UQ','2001 Max',\n",
    "                             '2011 Count','2011 Mean','2011 StD','2011 Min','2011 LQ','2011 Median','2011 UQ','2011 Max']\n",
    "\n",
    "# Useful, but time-consuming\n",
    "#plot_checks(d01_trs1, dsample, 'First-transform')\n",
    "\n",
    "descriptives_trs1[descriptives_trs1.index.isin(dsample)][\n",
    "    ['2001 Min','2011 Min','2001 Max','2011 Max','2001 Median','2011 Median','2001 Mean','2011 Mean']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptives_trs2 = pd.DataFrame()\n",
    "for c in d01_trs2.columns:\n",
    "    descriptives_trs2 = descriptives_trs2.append(pd.concat([d01_trs2[c].describe(),d11_trs2[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives_trs2.columns = ['2001 Count','2001 Mean','2001 StD','2001 Min','2001 LQ','2001 Median','2001 UQ','2001 Max',\n",
    "                             '2011 Count','2011 Mean','2011 StD','2011 Min','2011 LQ','2011 Median','2011 UQ','2011 Max']\n",
    "\n",
    "# Useful, but time-consuming\n",
    "#plot_checks(d01_trs2, dsample, 'Second-transform')\n",
    "\n",
    "descriptives_trs2[descriptives_trs2.index.isin(dsample)][\n",
    "    ['2001 Min','2011 Min','2001 Max','2011 Max','2001 Median','2011 Median','2001 Mean','2011 Mean']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tidy up\n",
    "del(s01, s11, d01, d11, d01input, d11input, d01_trs1, d11_trs1, rs1, rs2)\n",
    "del(dsample, descriptives, descriptives_trs1, descriptives_trs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code above has been run once, you do _not_ need to run it again -- _unless_ you want to change the transform used -- as we'll read the transformed data back from CSV in the next notebook. The files using different transforms do _not_ overwrite each other so as to make it easier to swap between approaches without needing to re-run this notebook multiple times."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ML Gentrification 3",
   "language": "python",
   "name": "mlgent3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
