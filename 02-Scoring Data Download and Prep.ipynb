{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Data Downloading & Preparation\n",
    "\n",
    "This notebook focusses on the 4 dimensions used for creating the scores for 2001 and 2011, and that we will try to predict for 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "r_state = 42\n",
    "random.seed(r_state) \n",
    "np.random.seed(r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/gent/lib/python3.7/site-packages/pysal/__init__.py:65: VisibleDeprecationWarning: PySAL's API will be changed on 2018-12-31. The last release made with this API is version 1.14.4. A preview of the next API version is provided in the `pysal` 2.0 prelease candidate. The API changes and a guide on how to change imports is provided at https://pysal.org/about\n",
      "  ), VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pysal as ps\n",
    "import libpysal as lps\n",
    "import requests\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "from scipy.stats import gmean\n",
    "from geoconvert import geoconvert\n",
    "\n",
    "lkp = os.path.join('data','lkp')\n",
    "src = os.path.join('data','src')\n",
    "\n",
    "canonical = os.path.join('data','canonical')\n",
    "converted = os.path.join(canonical,'converted')\n",
    "housing   = os.path.join(canonical,'housing')\n",
    "household = os.path.join(canonical,'households')\n",
    "work      = os.path.join(canonical,'work')\n",
    "\n",
    "for d in [canonical,converted,housing,household,work]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure you always run this!\n",
    "boroughs = ['City of London','Barking and Dagenham','Barnet','Bexley','Brent','Bromley',\n",
    "            'Camden','Croydon','Ealing','Enfield','Greenwich','Hackney','Hammersmith and Fulham',\n",
    "            'Haringey','Harrow','Havering','Hillingdon','Hounslow','Islington',\n",
    "            'Kensington and Chelsea','Kingston upon Thames','Lambeth','Lewisham',\n",
    "            'Merton','Newham','Redbridge','Richmond upon Thames','Southwark','Sutton',\n",
    "            'Tower Hamlets','Waltham Forest','Wandsworth','Westminster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have built London LSOA filter data for use where needed...\n",
      "\t2004: 4765 rows.\n",
      "\t2011: 4835 rows.\n"
     ]
    }
   ],
   "source": [
    "ldn2011 = pd.read_pickle(os.path.join(lkp,'LSOAs 2011.pkl'))\n",
    "ldn2004 = pd.read_pickle(os.path.join(lkp,'LSOAs 2004.pkl'))\n",
    "\n",
    "print(\"Have built London LSOA filter data for use where needed...\")\n",
    "print(\"\\t2004: \" + str(ldn2004.shape[0]) + \" rows.\")\n",
    "print(\"\\t2011: \" + str(ldn2011.shape[0]) + \" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_2011(df,src,dest,nm):\n",
    "    \"\"\"\n",
    "    Converts 2001 data to be compatible with 2011 data -- \n",
    "    this is to deal with the boundary changes that happen\n",
    "    at each Census.\n",
    "    \"\"\"\n",
    "    gc = geoconvert.geoconvert()\n",
    "    gc.auto_2001_to_2011(os.path.join(src,nm))\n",
    "\n",
    "    for f in glob.glob(re.sub(\"-\\d+\\.csv\",\"*\",nm)):\n",
    "        fn = re.sub(\"-converted\",\"\",f)\n",
    "        print(\"Moving \" + f + \" to \" + converted)\n",
    "        os.rename(f, os.path.join(converted,fn))\n",
    "    \n",
    "    dfc = pd.read_csv(os.path.join(converted,nm), index_col=False)\n",
    "    \n",
    "    dfc.columns=df.columns\n",
    "    \n",
    "    dfc.to_csv(os.path.join(dest,nm), index=False)\n",
    "    print(\"\\tConverted file has \" + str(dfc.shape[0]) + \" rows.\")\n",
    "    print(dfc.sample(2, random_state=r_state))\n",
    "    return\n",
    "\n",
    "def get_neighbours(ns, col):\n",
    "    \"\"\"\n",
    "    Find neighbours of a given LSOA.\n",
    "    \"\"\"\n",
    "    neighbours = []\n",
    "    for n in ns.keys():\n",
    "        #print(str(n) + \" -> \" + str(col[n][0][1]))\n",
    "        neighbours.append(col[n][0][1]) # Not elegant, but column name changes with year\n",
    "    return neighbours\n",
    "\n",
    "def get_gmean_from_neighbours(ns, prices):\n",
    "    \"\"\"\n",
    "    Find geometric mean of an LSOAs _neighbours'_ property transactions.\n",
    "    \"\"\"\n",
    "    print(\"\\tSearching for: \" + \", \".join(map(str, ns)))\n",
    "    medians = prices.loc[prices.index.isin(ns),'Median Property Price'].values\n",
    "    print(\"\\tFound median prices: \" + \", \".join(map(str, medians)))\n",
    "    return round(gmean(medians[np.logical_not(np.isnan(medians))]), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelled LSOA Household Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting LSOA Household Income estimates from London Data Store...\n",
      "Note: this has already been converted to use LSOA 2011 codes!\n",
      "Renaming...\n",
      "Converting to numeric data types...\n",
      "Have 4835 rows of data.\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting LSOA Household Income estimates from London Data Store...\")\n",
    "print(\"Note: this has already been converted to use LSOA 2011 codes!\")\n",
    "url  = ('https://files.datapress.com/london/dataset/'\n",
    "        'household-income-estimates-small-areas/'\n",
    "        'modelled-household-income-estimates-lsoa.csv')\n",
    "\n",
    "# Retrieve it\n",
    "hhi  = pd.read_csv(url, encoding='latin-1')\n",
    "\n",
    "# Rename key cols\n",
    "hhi.rename(columns={'Code':'lsoacd'}, inplace=True)\n",
    "\n",
    "hhi.set_index('lsoacd', inplace=True)\n",
    "\n",
    "# And break them down into subsets\n",
    "hhi2001 = hhi.loc[:,['Median 2001/02']]\n",
    "hhi2011 = hhi.loc[:,['Median 2011/12']]\n",
    "\n",
    "# Rename the columns\n",
    "print(\"Renaming...\")\n",
    "hhi2001.rename(columns=lambda x: x.replace(' 2001/02', ' Income'), inplace=True)\n",
    "hhi2011.rename(columns=lambda x: x.replace(' 2011/12', ' Income'), inplace=True)\n",
    "\n",
    "# Convert to numeric\n",
    "print(\"Converting to numeric data types...\")\n",
    "for df in [hhi2001, hhi2011]:\n",
    "    df.loc[:,('Median Income')] = pd.to_numeric(df.loc[:,'Median Income'].str.replace(\"\\D+\",\"\"), errors='coerce')\n",
    "\n",
    "# And save to CSV\n",
    "hhi2001.to_csv(os.path.join(work,'Income-2001.csv'), index=True, header=True, encoding='utf-8')\n",
    "hhi2011.to_csv(os.path.join(work,'Income-2011.csv'), index=True, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Have \" + str(hhi2001.shape[0]) + \" rows of data.\")\n",
    "print(\"   Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Housing & Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting LSOA Housing Value estimates from London Data Store...\n",
      "Note: this has already been converted to use LSOA 2011 codes!\n",
      "Have 4835 rows of data.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting LSOA Housing Value estimates from London Data Store...\")\n",
    "print(\"Note: this has already been converted to use LSOA 2011 codes!\")\n",
    "url  = ('https://files.datapress.com/london/dataset/'\n",
    "        'average-house-prices-ward-lsoa-msoa/' \n",
    "        '2016-07-06T14:34:00/house-prices-LSOAs.csv')\n",
    "\n",
    "# Retrieve it\n",
    "hhv  = pd.read_csv(url, na_values=\".\", encoding='latin-1')\n",
    "\n",
    "# Simplify column names\n",
    "hhv.rename(columns={\n",
    "        'Lower Super Output Area':'lsoacd',\n",
    "        'Names':'Name',\n",
    "        'Census 2011 dwellings':'Dwellings_2011'}, inplace=True)\n",
    "\n",
    "# Set the index\n",
    "hhv.set_index('lsoacd', inplace=True)\n",
    "hhv.rename(columns=lambda x: re.sub('-',' ',re.sub('(?:\\\\([^\\\\)]+\\\\))','',x)), inplace=True)\n",
    "\n",
    "\n",
    "# And break them down into subsets\n",
    "hhv2001 = hhv.loc[:,['Median 2001','Sales 1995',\n",
    "               'Sales 1996', 'Sales 1997', \n",
    "               'Sales 1998', 'Sales 1999',\n",
    "               'Sales 2000', 'Sales 2001']]\n",
    "hhv2011 = hhv.loc[:,['Median 2011','Sales 2005', \n",
    "               'Sales 2006', 'Sales 2007',\n",
    "               'Sales 2008', 'Sales 2009', \n",
    "               'Sales 2010', 'Sales 2011']]\n",
    "\n",
    "# Rename keys for consistency\n",
    "hhv2001.rename(columns={'Median 2001':'Median Property Price'}, inplace=True)\n",
    "hhv2011.rename(columns={'Median 2011':'Median Property Price'}, inplace=True)\n",
    "\n",
    "# Remove underscores\n",
    "hhv2001.rename(columns=lambda x: x.replace('_',''), inplace=True)\n",
    "hhv2011.rename(columns=lambda x: x.replace('_',''), inplace=True)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Have \" + str(hhv2001.shape[0]) + \" rows of data.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with NaNs\n",
    "\n",
    "It should be only the house price data that has NaNs -- I can't be 100% certain, but I'd assume that this is because there were no transactions in these LSOAs that year (and they don't -- and shouldn't -- fill in missing data by looking back at previous years) so there was nothing to report, or because those LSOAs didn't exist and they've not done a good job of back-filling with real data. \n",
    "\n",
    "We don't want to simply drop these areas from the analysis since they'll create gaps in our reuslts for no particularly good reason. Looking to the raw Land Registry data and then trying to work out the most representative range of nearby values would work but represents a huge amount of effort for relatively little return. Consequently, the most effective solution appears to me to take the geometric mean of the surrounding medians as a 'best guess' as to what values in the LSOA might be. The geometric mean is more robust to outliers and so should cope fairly well in those areas where there is a steep price gradient. But to make life easy you'll see below what values were used in each calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This data has already been mapped on to \n",
    "# 2011 LSOA boundaries... For some reason the\n",
    "# ones from the GLA Data Store don't work, but\n",
    "# the full one available from the OS do. \n",
    "qw       = ps.weights.Queen.from_shapefile(\n",
    "                os.path.join('data','shp','LSOA-Weights.shp')) # Weights/Adjacency\n",
    "fh       = ps.open(\n",
    "                os.path.join('data','shp','LSOA-Weights.dbf'))\n",
    "cds      = fh.by_col['lsoa11cd'] # LSOA 2011 Census code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001...\n",
      "\tLooking for neighbours of 27 areas without house prices.\n",
      "Finding neighbours for E01033573(id: 4760)\n",
      "\tSearching for: E01000154, E01000155, E01033572\n",
      "\tFound median prices: 117000.0, 93875.0, 178000.0\n",
      "\tE01033573 has been assigned geometric mean of neighbours: 125040.0\n",
      "Finding neighbours for E01000552(id: 541)\n",
      "\tSearching for: E01004708, E01004710, E01000544, E01000550, E01000551\n",
      "\tFound median prices: 210000.0, 156000.0, 150250.0, 227500.0, 220000.0\n",
      "\tE01000552 has been assigned geometric mean of neighbours: 189810.0\n",
      "Finding neighbours for E01000599(id: 586)\n",
      "\tSearching for: E01000600, E01033456\n",
      "\tFound median prices: 99000.0, 123500.0\n",
      "\tE01000599 has been assigned geometric mean of neighbours: 110570.0\n",
      "Finding neighbours for E01033424(id: 4739)\n",
      "\tSearching for: E01033457, E01000578, E01000615, E01000616, E01000617\n",
      "\tFound median prices: 147000.0, 83250.0, 125000.0, 154250.0, 133500.0\n",
      "\tE01033424 has been assigned geometric mean of neighbours: 125800.0\n",
      "Finding neighbours for E01033734(id: 4822)\n",
      "\tSearching for: E01033744, E01033731, E01033736, E01033742, E01001629\n",
      "\tFound median prices: 77995.0, 70000.0, 230000.0, 86000.0, 85000.0\n",
      "\tE01033734 has been assigned geometric mean of neighbours: 98300.0\n",
      "Finding neighbours for E01033735(id: 4823)\n",
      "\tSearching for: E01001705, E01033731, E01033737, E01001696, E01001699\n",
      "\tFound median prices: 92247.5, 83247.5, 94995.0, 70000.0, 94997.5\n",
      "\tE01033735 has been assigned geometric mean of neighbours: 86530.0\n",
      "Finding neighbours for E01033739(id: 4827)\n",
      "\tSearching for: E01033740, E01033741, E01033743\n",
      "\tFound median prices: 108995.0, 139495.0, 146995.0\n",
      "\tE01033739 has been assigned geometric mean of neighbours: 130740.0\n",
      "Finding neighbours for E01032789(id: 4705)\n",
      "\tSearching for: E01032790, E01001939\n",
      "\tFound median prices: 499000.0, 320000.0\n",
      "\tE01032789 has been assigned geometric mean of neighbours: 399600.0\n",
      "Finding neighbours for E01002089(id: 2039)\n",
      "\tSearching for: E01002082, E01001977, E01001980, E01002086\n",
      "\tFound median prices: 108000.0, 90000.0, 118250.0, 148000.0\n",
      "\tE01002089 has been assigned geometric mean of neighbours: 114200.0\n",
      "Finding neighbours for E01033030(id: 4711)\n",
      "\tSearching for: E01002590, E01033000, E01002593, E01002598, E01002612\n",
      "\tFound median prices: 136000.0, 125000.0, 125000.0, 127500.0, 125000.0\n",
      "\tE01033030 has been assigned geometric mean of neighbours: 127630.0\n",
      "Finding neighbours for E01033487(id: 4747)\n",
      "\tSearching for: E01033488, E01002765, E01002767\n",
      "\tFound median prices: 200000.0, 154000.0, 120000.0\n",
      "\tE01033487 has been assigned geometric mean of neighbours: 154610.0\n",
      "Finding neighbours for E01033493(id: 4753)\n",
      "\tSearching for: E01002752, E01033494, E01002749\n",
      "\tFound median prices: 272000.0, 220000.0, 199950.0\n",
      "\tE01033493 has been assigned geometric mean of neighbours: 228720.0\n",
      "Finding neighbours for E01002904(id: 2839)\n",
      "\tSearching for: E01001874, E01002852, E01001877, E01002905, E01002906, E01002907, E01002908\n",
      "\tFound median prices: 187975.0, 137500.0, 147000.0, 185000.0, 188750.0, nan, 275000.0\n",
      "\tE01002904 has been assigned geometric mean of neighbours: 182120.0\n",
      "Finding neighbours for E01002907(id: 2842)\n",
      "\tSearching for: E01002908, E01001874, E01002904\n",
      "\tFound median prices: 187975.0, 182120.0, 275000.0\n",
      "\tE01002907 has been assigned geometric mean of neighbours: 211150.0\n",
      "Finding neighbours for E01003184(id: 3116)\n",
      "\tSearching for: E01003181, E01003183, E01003185, E01003186, E01003187\n",
      "\tFound median prices: 151250.0, 199950.0, 177000.0, 233000.0, 223500.0\n",
      "\tE01003184 has been assigned geometric mean of neighbours: 194560.0\n",
      "Finding neighbours for E01003244(id: 3173)\n",
      "\tSearching for: E01003248, E01003249, E01003250\n",
      "\tFound median prices: 88000.0, 157500.0, 190000.0\n",
      "\tE01003244 has been assigned geometric mean of neighbours: 138090.0\n",
      "Finding neighbours for E01033574(id: 4761)\n",
      "\tSearching for: E01033575, E01033576, E01033577, E01033580\n",
      "\tFound median prices: nan, 100500.0, 219997.5, 225000.0\n",
      "\tE01033574 has been assigned geometric mean of neighbours: 170710.0\n",
      "Finding neighbours for E01033575(id: 4762)\n",
      "\tSearching for: E01003506, E01003609, E01003515, E01033574, E01033576, E01033580\n",
      "\tFound median prices: 115000.0, 98997.5, 136745.0, 170710.0, 100500.0, 225000.0\n",
      "\tE01033575 has been assigned geometric mean of neighbours: 134840.0\n",
      "Finding neighbours for E01033578(id: 4765)\n",
      "\tSearching for: E01033583, E01004224, E01033586, E01033579\n",
      "\tFound median prices: 119995.0, 130000.0, 87500.0, 123750.0\n",
      "\tE01033578 has been assigned geometric mean of neighbours: 114000.0\n",
      "Finding neighbours for E01003971(id: 3879)\n",
      "\tSearching for: E01003964, E01003966, E01003968, E01003970, E01003972, E01003974, E01003961\n",
      "\tFound median prices: 132000.0, 184000.0, 130000.0, 85000.0, 138250.0, 105000.0, 132000.0\n",
      "\tE01003971 has been assigned geometric mean of neighbours: 126360.0\n",
      "Finding neighbours for E01004011(id: 3915)\n",
      "\tSearching for: E01004007, E01004012, E01004013, E01004014, E01003912, E01003916\n",
      "\tFound median prices: 98500.0, 179995.0, 185000.0, 65000.0, 70878.0, 111000.0\n",
      "\tE01004011 has been assigned geometric mean of neighbours: 109000.0\n",
      "Finding neighbours for E01004268(id: 4164)\n",
      "\tSearching for: E01004231, E01004267, E01004269, E01004229\n",
      "\tFound median prices: 136250.0, 115000.0, 144500.0, 134995.0\n",
      "\tE01004268 has been assigned geometric mean of neighbours: 132220.0\n",
      "Finding neighbours for E01032771(id: 4687)\n",
      "\tSearching for: E01004277, E01032772, E01032773, E01004215, E01032774, E01032780, E01032782\n",
      "\tFound median prices: 177500.0, 306054.0, 141000.0, nan, 209000.0, 260435.0, 149950.0\n",
      "\tE01032771 has been assigned geometric mean of neighbours: 199220.0\n",
      "Finding neighbours for E01032773(id: 4689)\n",
      "\tSearching for: E01032772, E01032774, E01032770, E01032771\n",
      "\tFound median prices: 130000.0, 199220.0, 141000.0, 209000.0\n",
      "\tE01032773 has been assigned geometric mean of neighbours: 166210.0\n",
      "Finding neighbours for E01032779(id: 4695)\n",
      "\tSearching for: E01032780, E01032776, E01004221\n",
      "\tFound median prices: 190000.0, 246999.5, 260435.0\n",
      "\tE01032779 has been assigned geometric mean of neighbours: 230350.0\n",
      "Finding neighbours for E01033098(id: 4719)\n",
      "\tSearching for: E01004560, E01033100, E01033132, E01004566\n",
      "\tFound median prices: 219000.0, 247250.0, 127000.0, 130000.0\n",
      "\tE01033098 has been assigned geometric mean of neighbours: 172910.0\n",
      "Finding neighbours for E01033593(id: 4780)\n",
      "\tSearching for: E01004661, E01004702, E01033594, E01004681, E01033604\n",
      "\tFound median prices: 280000.0, 239000.0, 174000.0, 225000.0, 154000.0\n",
      "\tE01033593 has been assigned geometric mean of neighbours: 209490.0\n",
      " \n",
      "2011...\n",
      "\tLooking for neighbours of 13 areas without house prices.\n",
      "Finding neighbours for E01000422\n",
      "\tSearching for: E01000464, E01000466, E01001575, E01000470, E01001577, E01000416, E01000418\n",
      "\tFound median prices: 166000.0, 168500.0, 110000.0, 90500.0, 119500.0, 161000.0, 153500.0\n",
      "\tE01000422 has been assigned geometric mean of neighbours: 135120.0\n",
      "Finding neighbours for E01000605\n",
      "\tSearching for: E01033456, E01000596, E01000600, E01000604, E01000620, E01000621\n",
      "\tFound median prices: 245000.0, 180000.0, 237500.0, 282500.0, 271000.0, 230000.0\n",
      "\tE01000605 has been assigned geometric mean of neighbours: 238580.0\n",
      "Finding neighbours for E01001057\n",
      "\tSearching for: E01001078, E01001053, E01001054, E01001055, E01001056, E01001065\n",
      "\tFound median prices: 163000.0, 163000.0, 105000.0, 162000.0, 169000.0, 182500.0\n",
      "\tE01001057 has been assigned geometric mean of neighbours: 155130.0\n",
      "Finding neighbours for E01001510\n",
      "\tSearching for: E01001502, E01001506, E01001507, E01001509, E01001477\n",
      "\tFound median prices: 208000.0, 230000.0, 211500.0, 160000.0, 142500.0\n",
      "\tE01001510 has been assigned geometric mean of neighbours: 187330.0\n",
      "Finding neighbours for E01001713\n",
      "\tSearching for: E01001706, E01001708, E01001709, E01001710, E01001711, E01001712\n",
      "\tFound median prices: 108000.0, 316500.0, 129500.0, 163000.0, 120000.0, 88500.0\n",
      "\tE01001713 has been assigned geometric mean of neighbours: 140410.0\n",
      "Finding neighbours for E01001730\n",
      "\tSearching for: E01001735, E01001844, E01001850, E01001729, E01001731, E01001732\n",
      "\tFound median prices: 145000.0, 190000.0, 145614.0, 275000.0, 436750.0, 210000.0\n",
      "\tE01001730 has been assigned geometric mean of neighbours: 215870.0\n",
      "Finding neighbours for E01002082\n",
      "\tSearching for: E01001977, E01002084, E01002086, E01002088, E01002089, E01002095\n",
      "\tFound median prices: 187500.0, 241500.0, 288500.0, 310000.0, nan, 212500.0\n",
      "\tE01002082 has been assigned geometric mean of neighbours: 243760.0\n",
      "Finding neighbours for E01002089\n",
      "\tSearching for: E01002082, E01001977, E01001980, E01002086\n",
      "\tFound median prices: 187500.0, 242500.0, 243760.0, 288500.0\n",
      "\tE01002089 has been assigned geometric mean of neighbours: 237800.0\n",
      "Finding neighbours for E01002904\n",
      "\tSearching for: E01001874, E01002852, E01001877, E01002905, E01002906, E01002907, E01002908\n",
      "\tFound median prices: 385000.0, 190000.0, 265000.0, 325500.0, 527140.0, nan, 590000.0\n",
      "\tE01002904 has been assigned geometric mean of neighbours: 353830.0\n",
      "Finding neighbours for E01002907\n",
      "\tSearching for: E01002908, E01001874, E01002904\n",
      "\tFound median prices: 385000.0, 353830.0, 590000.0\n",
      "\tE01002907 has been assigned geometric mean of neighbours: 431550.0\n",
      "Finding neighbours for E01003244\n",
      "\tSearching for: E01003248, E01003249, E01003250\n",
      "\tFound median prices: 138500.0, 212500.0, 274500.0\n",
      "\tE01003244 has been assigned geometric mean of neighbours: 200660.0\n",
      "Finding neighbours for E01003337\n",
      "\tSearching for: E01004002, E01004016, E01003231, E01003232, E01003336\n",
      "\tFound median prices: 272500.0, 383250.0, 180250.0, 389750.0, 330000.0\n",
      "\tE01003337 has been assigned geometric mean of neighbours: 299780.0\n",
      "Finding neighbours for E01003971\n",
      "\tSearching for: E01003964, E01003966, E01003968, E01003970, E01003972, E01003974, E01003961\n",
      "\tFound median prices: 194000.0, 441500.0, 193500.0, 138000.0, 264400.0, 215000.0, 125000.0\n",
      "\tE01003971 has been assigned geometric mean of neighbours: 206940.0\n",
      " \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"2001...\")\n",
    "nan01 = hhv2001[hhv2001['Median Property Price'].isnull()].index.values\n",
    "print(\"\\tLooking for neighbours of \" + str(len(nan01)) + \" areas without house prices.\")\n",
    "\n",
    "for z in nan01:\n",
    "    print(\"Finding neighbours for \" + z + \"(id: \" + str(cds.index(z)) + \")\")\n",
    "    neighbours01 = get_neighbours(qw[ cds.index(z) ], fh)\n",
    "    \n",
    "    m = get_gmean_from_neighbours(neighbours01, hhv2001)\n",
    "    print(\"\\t\" + z + \" has been assigned geometric mean of neighbours: \" + str(m))\n",
    "    hhv2001.loc[z,'Median Property Price'] = m\n",
    "print(\" \")\n",
    "\n",
    "print(\"2011...\")\n",
    "nan11 = hhv2011[hhv2011['Median Property Price'].isnull()].index.values\n",
    "print(\"\\tLooking for neighbours of \" + str(len(nan11)) + \" areas without house prices.\")\n",
    "\n",
    "for z in nan11:\n",
    "    print(\"Finding neighbours for \" + z)\n",
    "    neighbours11 = get_neighbours(qw[ cds.index(z) ], fh)\n",
    "    \n",
    "    m = get_gmean_from_neighbours(neighbours11, hhv2011)\n",
    "    print(\"\\t\" + z + \" has been assigned geometric mean of neighbours: \" + str(m))\n",
    "    hhv2011.loc[z,'Median Property Price'] = m\n",
    "print(\" \")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Median Property Price</th>\n",
       "      <th>Sales 1995</th>\n",
       "      <th>Sales 1996</th>\n",
       "      <th>Sales 1997</th>\n",
       "      <th>Sales 1998</th>\n",
       "      <th>Sales 1999</th>\n",
       "      <th>Sales 2000</th>\n",
       "      <th>Sales 2001</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsoacd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01001510</th>\n",
       "      <td>80000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Median Property Price  Sales 1995  Sales 1996  Sales 1997  \\\n",
       "lsoacd                                                                 \n",
       "E01001510                80000.0           0           0           0   \n",
       "\n",
       "           Sales 1998  Sales 1999  Sales 2000  Sales 2001  \n",
       "lsoacd                                                     \n",
       "E01001510           0           2           2           4  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should have been pulled from real data\n",
    "hhv2001[hhv2001.index=='E01001510']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Median Property Price</th>\n",
       "      <th>Sales 2005</th>\n",
       "      <th>Sales 2006</th>\n",
       "      <th>Sales 2007</th>\n",
       "      <th>Sales 2008</th>\n",
       "      <th>Sales 2009</th>\n",
       "      <th>Sales 2010</th>\n",
       "      <th>Sales 2011</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsoacd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E01001510</th>\n",
       "      <td>187330.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Median Property Price  Sales 2005  Sales 2006  Sales 2007  \\\n",
       "lsoacd                                                                 \n",
       "E01001510               187330.0           2           3           6   \n",
       "\n",
       "           Sales 2008  Sales 2009  Sales 2010  Sales 2011  \n",
       "lsoacd                                                     \n",
       "E01001510           2           4           3           0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should have been assigned from the geometric mean of neighbours calculation\n",
    "hhv2011[hhv2011.index=='E01001510']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 4835 rows of data.\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "# And save to CSV\n",
    "hhv2001.loc[:,['Median Property Price']].to_csv(os.path.join(housing,'Values-2001.csv'), index=True, header=True, encoding='utf-8')\n",
    "hhv2011.loc[:,['Median Property Price']].to_csv(os.path.join(housing,'Values-2011.csv'), index=True, header=True, encoding='utf-8')\n",
    "\n",
    "# Probably not useful but worked out just in case\n",
    "# the rate of transactions in the runup to the \n",
    "# Census year is a useful indicator.\n",
    "hhv2001.loc[:,['Sales 1995','Sales 1996', 'Sales 1997', 'Sales 1998', 'Sales 1999',\n",
    "               'Sales 2000', 'Sales 2001']].to_csv(os.path.join(housing,'Transactions-2001.csv'), index=True, header=True, encoding='utf-8')\n",
    "hhv2011.loc[:,['Sales 2005', 'Sales 2006', 'Sales 2007','Sales 2008', 'Sales 2009', \n",
    "               'Sales 2010', 'Sales 2011']].to_csv(os.path.join(housing,'Transactions-2011.csv'), index=True, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Have \" + str(hhv2001.shape[0]) + \" rows of data.\")\n",
    "print(\"   Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2001 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Processing 2001 Occupations data from Nomis Table KS012a...\")\n",
    "print(\"Note: this needs to be converted to LSOA 2011 codes using GeoConvert!\")\n",
    "\n",
    "# Load the data from the KS012a table\n",
    "occ_01 = pd.read_csv(os.path.join(src,\"2001\",\"ks012a.csv.gz\"),\n",
    "                      header=5, skip_blank_lines=True, compression='gzip')\n",
    "\n",
    "# Rename the columns to something easier to work with\n",
    "occ_01.rename(columns=lambda x: re.sub(\"^\\d+\\. \",\"\",x), inplace=True)\n",
    "occ_01.rename(columns={\n",
    "    'mnemonic':'lsoacd', \n",
    "    'super output areas - lower layer':'LSOANM', \n",
    "    'All categories: Occupation':'Total',\n",
    "    'Managers and senior officials':'Managerial',\n",
    "    'Professional occupations':'Professional',\n",
    "    'Associate professional and technical occupations':'Technical',\n",
    "    'Administrative and secretarial occupations':'Administrative',\n",
    "    'Skilled trades occupations':'Skilled',\n",
    "    'Personal service occupations':'Personal Service',\n",
    "    'Sales and customer service occupations':'Customer Service',\n",
    "    'Process, plant and machine operatives':'Operators',\n",
    "    'Elementary occupations':'Elementary'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only those rows that are in the London 2001 LSOA list\n",
    "occ_01 = occ_01.loc[occ_01.lsoacd.isin(ldn2004.lsoacd.values)]\n",
    "\n",
    "# Drop the columns we're not interested in\n",
    "occ_01.drop('LSOANM', axis=1, inplace=True)\n",
    "\n",
    "occ_01.to_csv(os.path.join(src,\"Occupations-2001.csv\"), index=False, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Wrote \" + str(occ_01.shape[0]) + \" rows to output file.\")\n",
    "\n",
    "# convert_to_2011(df,src,dest,nm)\n",
    "convert_to_2011(occ_01, src, work, 'Occupations-2001.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Processing 2011 Occupations data from Nomis Table KS610EW...\")\n",
    "print(\"Note: this does not need to be converted.\")\n",
    "\n",
    "# Load the data from the KS610EW table\n",
    "occ_11 = pd.read_csv(os.path.join(src,\"2011\",\"ks610ew.csv.gz\"),\n",
    "                      header=7, skip_blank_lines=True, compression='gzip')\n",
    "\n",
    "# Rename the columns to something easier to work with\n",
    "occ_11.rename(columns=lambda x: re.sub(\"^\\d+\\. \",\"\",x), inplace=True)\n",
    "occ_11.rename(columns={\n",
    "    'mnemonic':'lsoacd', \n",
    "    '2011 super output area - lower layer':'LSOANM', \n",
    "    'All categories: Occupation':'Total',\n",
    "    'Managers, directors and senior officials':'Managerial',\n",
    "    'Professional occupations':'Professional',\n",
    "    'Associate professional and technical occupations':'Technical',\n",
    "    'Administrative and secretarial occupations':'Administrative',\n",
    "    'Skilled trades occupations':'Skilled',\n",
    "    'Caring, leisure and other service occupations':'Personal Service',\n",
    "    'Sales and customer service occupations':'Customer Service',\n",
    "    'Process plant and machine operatives':'Operators',\n",
    "    'Elementary occupations':'Elementary'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only those rows that are in the London 2011 LSOA list\n",
    "occ_11 = occ_11.loc[occ_11.lsoacd.isin(ldn2011.lsoacd.values)]\n",
    "\n",
    "# Drop the columns we're not interested in\n",
    "occ_11.drop('LSOANM', axis=1, inplace=True)\n",
    "occ_11.to_csv(os.path.join(work,\"Occupations-2011.csv\"), index=False, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Wrote \" + str(occ_11.shape[0]) + \" rows to output file.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2001 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Processing 2001 Qualifications data from Nomis Table KS013...\")\n",
    "print(\"Note: this needs to be converted to LSOA 2011 codes using GeoConvert!\")\n",
    "\n",
    "# Load the data from the KS013 table\n",
    "quals_01 = pd.read_csv(os.path.join(src,\"2001\",\"ks013.csv.gz\"),\n",
    "                      header=5, skip_blank_lines=True, compression='gzip')\n",
    "\n",
    "# Rename the columns to something easier to work with\n",
    "quals_01.rename(columns=lambda x: re.sub(\"(?:Highest level of qualification: )(.+) qualifications\",\"\\\\1\",x), inplace=True)\n",
    "quals_01.rename(columns=lambda x: re.sub(\"(?:Full-time students: Age 18 to 74: Economically )(?:active: )?(.+)\",\"Students: \\\\1\",x), inplace=True)\n",
    "quals_01.rename(columns={\n",
    "    'mnemonic':'lsoacd', \n",
    "    'super output areas - lower layer':'LSOANM', \n",
    "    'All people aged 16-74':'Total'}, inplace=True)\n",
    "\n",
    "# Select only those rows that are in the London 2001 LSOA list\n",
    "quals_01 = quals_01.loc[quals_01.lsoacd.isin(ldn2004.lsoacd.values)]\n",
    "\n",
    "# Drop the columns we're not interested in\n",
    "quals_01.drop('LSOANM', axis=1, inplace=True)\n",
    "\n",
    "quals_01.to_csv(os.path.join(src,\"Qualifications-2001.csv\"), index=False, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Wrote \" + str(quals_01.shape[0]) + \" rows to output file.\")\n",
    "\n",
    "# convert_to_2011(df,src,dest,nm)\n",
    "convert_to_2011(quals_01, src, work, 'Qualifications-2001.csv')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011 Data\n",
    "\n",
    "Note that we don't make use of the 'Apprenticeship' column as it has no equivalent in the 2001 data and we need a comparable base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Processing 2011 Qualifications data from Nomis Table KS501EW...\")\n",
    "print(\"Note: this does not need to be converted.\")\n",
    "\n",
    "# Load the data from the KS501EW table\n",
    "quals_11 = pd.read_csv(os.path.join(src,\"2011\",\"ks501ew.csv.gz\"),\n",
    "                      header=6, skip_blank_lines=True, compression='gzip')\n",
    "\n",
    "# Rename the columns to something easier to work with\n",
    "quals_11.rename(columns=lambda x: re.sub(\"(?:Highest level of qualification: )(.+) qualifications\",\"\\\\1\",x), inplace=True)\n",
    "quals_11.rename(columns=lambda x: re.sub(\"(?:Full-time students: Age 18 to 74: Economically )(?:active: )?(.+)\",\"Students: \\\\1\",x), inplace=True)\n",
    "quals_11.rename(columns={'mnemonic':'lsoacd', '2011 super output area - lower layer':'LSOANM', 'All categories: Highest level of qualification':'Total'}, inplace=True)\n",
    "\n",
    "# Select only those rows that are in the London 2011 LSOA list\n",
    "quals_11 = quals_11.loc[quals_11.lsoacd.isin(ldn2011.lsoacd.values)]\n",
    "\n",
    "# Drop the columns we're not interested in -- although it\n",
    "# would be nice to keep the Apprenticeship data we can't\n",
    "# seemingly compare it to the 2001 data. As far as I can tell\n",
    "# this is because the question was new in 2011, so presumably\n",
    "# respondents in 2001 would have been folded into one of the \n",
    "# 'lower' qualifications brackets. For a brief analysis, see\n",
    "# https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/articles/qualificationsandlabourmarketparticipationinenglandandwales/2014-06-18\n",
    "quals_11.drop(['LSOANM','Highest level of qualification: Apprenticeship'], axis=1, inplace=True)\n",
    "\n",
    "quals_11.to_csv(os.path.join(work,\"Qualifications-2011.csv\"), index=False, header=True, encoding='utf-8')\n",
    "\n",
    "# Sanity check\n",
    "print(\"Wrote \" + str(quals_11.shape[0]) + \" rows to output file.\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rents\n",
    "\n",
    "There is some generic data on rents that might be useful, but unfortunately no one seems to have statistics as far back as 2001 -- the earliest I could find dated to 2014 and the VOA specifically recommends against trying to compare across years with much of their data:\n",
    "\n",
    "- [VOA Home Page @ National Archives](http://webarchive.nationalarchives.gov.uk/20141002130950/http://www.voa.gov.uk/corporate/index.html)\n",
    "- [Private Rental Market Stats @ National Archives](http://webarchive.nationalarchives.gov.uk/20141002135606/http://www.voa.gov.uk/corporate/statisticalReleases/110929_PrivateResidentialRentalMarketStatistics.html)\n",
    "- [General VOA Stats Page @ National Archives](http://webarchive.nationalarchives.gov.uk/20141002132258/http://www.voa.gov.uk/corporate/publications/statistics.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Ml Gent Debug",
   "language": "python",
   "name": "gent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
